# config for crawl and extract


start_urls: # the urls start to crawl.
  - # url 1
  - # url 2

allowed_domains:
  - # domain 1
  - # domain 2

code_file: code_template.py

# extract rule: path_type{path}.func().func()..
# - path_type: css, xpath, json, re, code
# - path: css selector, xpath, jsonpath, re rule, python code
# - func: replace, split, re
# examples:
# - css{div a::attr(href)}, css{div a::text}
# - xpath{../a/@href}, xpath{../a/text()}
# - json{$.data.title}
# - re{"(.*)"}
# - code{}
extract:
  - page:
      page_type: seed_page
      url_patterns:  # re pattern list, the matched url will be extracted by this page config.
        - url re pattern 1
        - url re pattern 2
      list_rule: # extract list item rule(css, xpath, json, re...)
      next_page_url: # extract next page url rule(css, xpath, json, re...)
      item_rules:
        name1: # extract name1 for each item with rule(css, xpath, json, re...)
        name2: # extract name2 for each item with rule(css, xpath, json, re...)
      code: # optional
        '  
        seed_customize(response, results, next_page_url)
        '

  - page:
      page_type: list_page
      url_patterns: # re pattern list, the matched url will be extracted by this page config.
        - url re pattern 1
        - url re pattern 2
      list_rule: # extract list item rule(css, xpath, json, re...)
      next_page_url: # extract next page url rule(css, xpath, json, re...)
      item_rules: # if item['url'] and go_detail: the item['url'] will be crawled.
        name1: # extract name1 for each item with rule(css, xpath, json, re...)
        name2: # extract name2 for each item with rule(css, xpath, json, re...)
      code: '
      list_customize(response, results, next_page_url)
      '

  - page:
      page_type: detail_page
      url_patterns: # re pattern list, the matched url will be extracted by this page config.
        - url re pattern 1
        - url re pattern 2
      list_rule: # extract list item rule(css, xpath, json, re...)
      next_page_url: # extract next page url rule(css, xpath, json, re...)
      item_rules:
        name1: # extract name1 for each item with rule(css, xpath, json, re...)
        name2: # extract name2 for each item with rule(css, xpath, json, re...)
      link_params: #https://docs.scrapy.org/en/latest/topics/link-extractors.html
        restrict_css: <css selector>
        allow_domains:
          - domain1
          - domain2
      code: '
      detail_customize(response, results, next_page_url)
      '


settings: # replace default settings  https://docs.scrapy.org/en/latest/topics/settings.html
  USER_AGENT: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36
  LOG_LEVEL: INFO
  DOWNLOAD_DELAY: 1
  DEFAULT_REQUEST_HEADERS:
    Accept": text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
    Accept-Language: en

#proxies:
#  - http://1
#  - http://2


# login method 1: login through login page form
#login:
#  url: login page
#  load_page: True # default True
#  params:
#    formcss: xxx
#    formxpath: xxx
#    formname: xxx
#    formid: xxx
#  form_data:
#    name: xxx
#    password: xxx

# login method 2: login through login api
#login:
#  url: login api
#  load_page: False
#  form_data:
#    name: xxx
#    password: xxx

# login method 3: login through cookie
#settings:
#  COOKIES_ENABLED: False
#  DEFAULT_REQUEST_HEADERS:
#    cookie: xxxx